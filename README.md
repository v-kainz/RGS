# The Reputation Game Simulation
A theoretical model for communication dynamics focusing on reputation. Based on a minimal number of basic mathematical and physical principles, we try to create a socio-physical model of interpersonal communication that is as realistic as possible. In this agent-based simulation we rebuild stereotypical personality traits and study their influence on their own and others' reputations. Especially the Theory of Mind that the agents use to assess others and judge their statements, as well as their thinking, i.e. their way of processing knowledge are in the center of our investigations.

## Table of Contents
* [Overview](#overview)
* [Key Features](#key-features)
* [This version](#this-version)
* [Installation](#installation)
* [Usage](#usage)

## Overview
The reputaion game simulation (RGS) is an agent-based model that simulates one-to-one communications between agents and the development of their mutual knowledge about each other. A fixed number of agents go through a predefined number of rounds of conversation, where each agent initiates exactly one conversation to another agent, i.e. determines an interlocutor, as well as a topic of conversation and transmits a corresponding (not necessarily honest) message. The interlocutor then has to answer about the same topic. After both conversation participants have given their opinion, both perform an update by incorporating the newly received information into their state of knowledge. The actions that an agent can perform are to choose an interlocutor, to choose a topic of conversation, and, in the case of a lie, to design the message to be delivered. For all these choices, different strategies can be applied that can produce different dynamics in the opinions of all the agents involved. We have therefore replicated different stereotypical behaviors and observed their influence on themselves and the other agents. In this way, we are able to reproduce some real socio-psychological effects that appear as emergent phenomena in the reputation game simulations.\
More details about this, as well as about the overall operation and mathematical concepts behind the reputation game, can be found in [this paper](https://onlinelibrary.wiley.com/doi/full/10.1002/andp.202100277). A shorter version of it with more high-level explanations is available [here](https://www.aup-online.com/content/journals/10.5117/CCR2023.1.9.ENSS/). 

## Key Features
A fundamental part of each agent-based communication simulation is of course the knowledge the agents have about others and the way this knowledge is obtained during their interactions. Two other key features to which we put particular importance when developing the reputation game are a working Theory of Mind and realistic information processing, i.e. thingking of the agents.
### Knowledge
The knowledge that each agent possesses about the others is, in the case of the reputation game, the respective presumed honesty of the other agents. The average estimated honesty of an agent is thus defined as its reputation, since the consequences of a high reputation and an agent's presumed high honesty are very similar. For example, both the acceptability of a statement, and thus its influence, and the tolerance towards an agent correlate with that agent's apparent honesty, or reputation. Opinions about the honesty of others are represented by continuous values between 0 and 1 and are stored as a parameterized probability distribution in the agents' memories. In this way, not only the mean value, i.e. the respective opinion, is accessible, but also the uncertainty around it. Especially for rational reasoning, the uncertainty of one's own opinion, but also of the received opinion, is important, since different consequences might have to be drawn accordingly. In addition to the two opinions at hand together with their uncertainties, several other factors are also involved in the agents' reasoning processes. Depending on the mindset of an agent this might include its current opinion about the speaker in general, the difference between the two opinions, clear signs like blushing in case of a lie or the the presence of a confession, as well as the assumed intention of the speaker. Finally, taking all these factors into account, a bayesian update is performed, whereby the receiver of a message incorporates new information into their knowledge both about the subject of the message and about its sender.
### Theory of Mind
A ToM enables people to pick up information about the thoughts and states of others. It is therefore essential in interpersonal relationships to be able to understand other people's view and to act accordingly. At the same time, a ToM is a valuable tool when it comes to deceiving or manipulating others, because only if the state of the counterpart is known, this can be influenced purposefully. If you go one level deeper, the ToM can of course also serve to recognize such deceptive intentions and thus help to be less susceptible. Therefore you see, no matter what kind of interpersonal relationship we are talking about, the ability to find out states of mind, thoughts and goals of others is extremely important and omnipresent in every social interaction. In the reputation game, we therefore have introduced a memory of the presumed opinions of others, which is updated after each communication. Unlike for the direct opinions of the agents themselves, a slightly modified version of DeGroot learning is currently used for their ToM updates, where the weights assigned to the statements of the other agents are not constant, but depend on the respective message, as well as further information about the speaker. A fully bayesian learning process in the ToM similarly to the agents' direct opinions will be implemented in the near future.
### Mental limitations
At first, it may seem remarkable to speak of mental limitations, when it is precisely the reasoning processes and the acquisition of knowledge about others that are in the foreground in the reputation game simulation. But if one considers the sheer amount of information that would be necessary to assign a probability value to every single configuration of reality, one quickly encounters a quite expensive regime. But not only for computers the amount of accessible information quickly becomes unmanageable, but also for humans such a cognitive performance limit exists. Obviously, we do not remember every part of every situation ever experienced, but instead apply highly efficient subconscious strategies to filter out rough impressions, opinions and feelings from experienced situations. Especially in interpersonal encounters, such mental shortcuts are extremely important to grasp the most important features of a situation in a split second and to be able to act as empathetically and flexibly as we are used to. Thus, the agents in the RGS should not function like perfect logical thinking machines, but compress the detailed information available to them after each conversation to an abstracted, less complex and faster to process opinion. However, to make this process not random, but to ensure that agents retain the essential elements in their memory, we use the principle of minimal information loss. The agents retain as much useful information as possible, but reduce their knowledge to a rough impression, or gut feeling.

## This Version
This version of the RGS especially focuses on learning the evidential value of credibility signs given by other agents. Thus, the blushing frequencies (although they are still fixed in the simulation setup) are no longer given to the agents, but have to be learned from their experiences throughout the simulation. In the code, this feature is often referred to as EBEH (experiences based evidence handling). Also, we introduced the possibility to continue previously ran simulations either with the exact same state as they ended or by updating certain parameters in between and continuing from there.

## Installation
### Requirements
Install all requirements via `pip install -r requirements.txt`.

### Get it run
Start your first RGS by entering `python main.py` into the terminal. You will see that a new folder `experiments` and a subfolder `LICS` are created and a number of simulations with three ordinary agents over 300 conversation rounds each are started. Each simulation will thereby create a json file that shows the individual processes in this simulation, i.e. who said and learned what at which time. You will also get a figure that shows the average reputation of the three agents as a function of time. To obtain some more results, especially relevant to the study of the LICS (Learned Insignificance of Credibility Signs) effect you can run `python main_evaluation.py` that generates more figures into the same folder.

## Usage
### Overview
As a user you prpbably can focus on running the file **main.py**. To specify the parameters of simulations, modify the configuration file **config/config.yml**. There you can change for example the number of participating agents, their strategies, where the results should be stored, the statistic-size, as well as many technical parameters. If you have already run a simulation with the default settings, you will have seen that the simulations have been evaluated graphically as plots. To specify more precisely which plots should be generated, i.e. which quantity should be considered in the evaluation, use the **config/config_evaluation.yml** file. There you can turn on and off all plots individually by setting their value to `True`or `False` and run the **main_evaluation.py** script again. Make sure you specify the right folder, which should be evaluated at the top of the config file.\
With the file **main_continue.py** previously ran simulations can be continued. Be careful: the random number choices are not continously resumed, such that two simulations where one originally had 300 rounds and another simulation that had 200 rounds and was continued to 300 do not show the same results in the last 100 rounds. However, the statistical properties of both simulations are the same. Specify the simulation that you want to continue in the **config/config_continue.yml** file, as well as certain parameters that you might want to reset to specific values before the continuation.

### Useful Details
**How to specify the agents' strategies, i.e. the simulation mode:**
- for each simulation specify one dictionary
- for example: `{'0': 'dominant', '2': 'manipulative'}` i.e. agent 0 plays the dominant strategy and agent 2 the manipulative one. 
- all agents that are not explicitely named here are ordinary agents. 
- to change the strategy of all agents use the key 'all', e.g. `{'all':'ordinary', '0':'dominant'}`
- if you want to start more simulations at once just give a list more dictionaries, e.g. `modes = [{'0': 'dominant', '2': 'manipulative'}, {'0': 'dominant', '2': 'destructive'}]`

**Most commonly used strategies are:**
- ordinary, manipulative, dominant or destructive

For the investigation of the LICS we only considered simulations with 3 agents, each having one special agent and two ordinary agents. The special agents use the strategies manipulative, dominant or destructive. I.e. the modes would be `[{'all':'ordinary'}, {'0':'manipulative'}, {'0':'dominant'}, {'0':'destructive'}]`.\
Hint: do not change any other than the "game parameters" unless you know what you are doing.



